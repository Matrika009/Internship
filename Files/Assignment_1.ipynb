{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae3bb0b",
   "metadata": {},
   "source": [
    "# Question1 --> Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1354cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "                            Head\n",
      "0           Welcome to Wikipedia\n",
      "1  From today's featured article\n",
      "2               Did you know ...\n",
      "3                    In the news\n",
      "4                    On this day\n",
      "5       Today's featured picture\n",
      "6       Other areas of Wikipedia\n",
      "7    Wikipedia's sister projects\n",
      "8            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "titles = []\n",
    "for i in soup.find_all(\"span\",class_=\"mw-headline\"):\n",
    "    titles.append(i.text)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Head\":titles})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177d609",
   "metadata": {},
   "source": [
    "# Question2 -->Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7923aefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "                            Name           Term of Office\n",
      "0           Shri Ram Nath Kovind  14th President of India\n",
      "1          Shri Pranab Mukherjee  13th President of India\n",
      "2   Smt Pratibha Devisingh Patil  12th President of India\n",
      "3         DR. A.P.J. Abdul Kalam  11th President of India\n",
      "4           Shri K. R. Narayanan  10th President of India\n",
      "5        Dr Shankar Dayal Sharma  9th  President of India\n",
      "6            Shri R Venkataraman   8th President of India\n",
      "7               Giani Zail Singh   7th President of India\n",
      "8      Shri Neelam Sanjiva Reddy   6th President of India\n",
      "9       Dr. Fakhruddin Ali Ahmed   5th President of India\n",
      "10  Shri Varahagiri Venkata Giri   4th President of India\n",
      "11              Dr. Zakir Husain   3rd President of India\n",
      "12  Dr. Sarvepalli Radhakrishnan   2nd President of India\n",
      "13           Dr. Rajendra Prasad   1st President of India\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get(\"https://presidentofindia.nic.in/former-presidents\")\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "titles = []\n",
    "for i in soup.find_all(\"div\",class_=\"desc-sec\"):\n",
    "    titles.append(i.text.strip())\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Head\":titles})\n",
    "df[['Name', 'Term of Office']] = df['Head'].str.split('\\n', expand=True)\n",
    "df.drop(columns=['Head'], inplace=True)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750563b8",
   "metadata": {},
   "source": [
    "# Question3 -->Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53dd4ba",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0a403c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "           Team Matches Points Ratings\n",
      "0         India      49  5,839     119\n",
      "1     Australia      36  4,015     112\n",
      "2      Pakistan      32  3,525     110\n",
      "3  South Africa      29  3,166     109\n",
      "4   New Zealand      38  4,007     105\n",
      "5       England      34  3,377      99\n",
      "6     Sri Lanka      43  3,943      92\n",
      "7    Bangladesh      40  3,574      89\n",
      "8   Afghanistan      26  2,170      83\n",
      "9   West Indies      38  2,582      68\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "team = []\n",
    "for i in soup.find_all(\"span\",class_=\"u-hide-phablet\"):\n",
    "    team.append(i.text.strip())\n",
    "\n",
    "\n",
    "matches = []\n",
    "for i in soup.find_all(\"td\",class_=\"rankings-block__banner--matches\"):\n",
    "    matches.append(i.text.strip())\n",
    "test = []\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell u-center-text\"):\n",
    "    test.append(i.text.strip())\n",
    "for x,y in enumerate(test):\n",
    "    if x%2==0:\n",
    "        matches.append(y)\n",
    "\n",
    "\n",
    "\n",
    "points = []\n",
    "for i in soup.find_all(\"td\",class_=\"rankings-block__banner--points\"):\n",
    "    points.append(i.text.strip())\n",
    "test = []\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell u-center-text\"):\n",
    "    test.append(i.text.strip())\n",
    "for x,y in enumerate(test):\n",
    "    if x%2!=0:\n",
    "        points.append(y)\n",
    "\n",
    "\n",
    "ratings = []\n",
    "for i in soup.find_all(\"td\",class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "    ratings.append(i.text.strip())\n",
    "\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell u-text-right rating\"):\n",
    "    ratings.append(i.text.strip())\n",
    "        \n",
    "        \n",
    "        \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Team\":team[:10],\"Matches\":matches[:10],\"Points\":points[:10],\"Ratings\":ratings[:10]})\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ffa4a",
   "metadata": {},
   "source": [
    "b) Top 10 ODI Batsmen along with the records of their team andrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "41481f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "                    Team Ratings\n",
      "0             Babar Azam     829\n",
      "1           Shubman Gill     823\n",
      "2        Quinton de Kock     769\n",
      "3       Heinrich Klaasen     756\n",
      "4           David Warner     747\n",
      "5            Virat Kohli     747\n",
      "6           Harry Tector     729\n",
      "7           Rohit Sharma     725\n",
      "8  Rassie van der Dussen     716\n",
      "9            Imam-ul-Haq     704\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\")\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "team1 = []\n",
    "for i in soup.find_all(\"div\",class_=\"rankings-block__banner--name\"):\n",
    "    team1.append(i.text.strip())\n",
    "team = team1[0]\n",
    "team2 = [team]\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell name\"):\n",
    "    team2.append(i.text.strip())\n",
    "\n",
    "\n",
    "    \n",
    "rating1 = []\n",
    "for i in soup.find_all(\"div\",class_=\"rankings-block__banner--rating\"):\n",
    "    rating1.append(i.text.strip())\n",
    "rating = rating1[0]\n",
    "rating2 = [rating]\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell u-text-right rating\"):\n",
    "    rating2.append(i.text.strip())\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Team\":team2[:10],\"Ratings\":rating2[:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c9af9",
   "metadata": {},
   "source": [
    " c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "94b02360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "             Team Ratings\n",
      "0  Josh Hazlewood     670\n",
      "1  Mohammed Siraj     668\n",
      "2  Keshav Maharaj     656\n",
      "3     Rashid Khan     654\n",
      "4     Trent Boult     653\n",
      "5   Mohammad Nabi     641\n",
      "6      Adam Zampa     635\n",
      "7      Matt Henry     634\n",
      "8   Kuldeep Yadav     632\n",
      "9  Shaheen Afridi     625\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\")\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "team1 = []\n",
    "for i in soup.find_all(\"div\",class_=\"rankings-block__banner--name\"):\n",
    "    team1.append(i.text.strip())\n",
    "filtered_list = list(filter(lambda x: x == team1[1], team1))\n",
    "uni_list=list(set(filtered_list))\n",
    "\n",
    "team2 = []\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell name\"):\n",
    "    team2.append(i.text.strip())\n",
    "team3 = team2[9:18]\n",
    "F_team = uni_list+team3\n",
    "\n",
    "\n",
    "\n",
    "rating1 = []\n",
    "for i in soup.find_all(\"div\",class_=\"rankings-block__banner--rating\"):\n",
    "    rating1.append(i.text.strip())\n",
    "    \n",
    "    \n",
    "filtered_rat = list(filter(lambda x: x == rating1[1], rating1))\n",
    "uni_rat=list(set(filtered_rat))    \n",
    "\n",
    "\n",
    "rating2 = []\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell u-text-right rating\"):\n",
    "    rating2.append(i.text.strip())\n",
    "rat3 = rating2[9:18]\n",
    "F_rat = uni_rat+rat3\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Team\":F_team,\"Ratings\":F_rat})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41679a29",
   "metadata": {},
   "source": [
    "# Question 4 -Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8baa3",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "897af0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "           Team Matches Points Ratings\n",
      "0     Australia      19  3,084     162\n",
      "1       England      23  2,991     130\n",
      "2  South Africa      21  2,446     116\n",
      "3         India      18  1,745      97\n",
      "4   New Zealand      21  2,014      96\n",
      "5   West Indies      18  1,610      89\n",
      "6     Sri Lanka       9    714      79\n",
      "7    Bangladesh      11    816      74\n",
      "8      Thailand      11    753      68\n",
      "9      Pakistan      21  1,435      68\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "team = []\n",
    "for i in soup.find_all(\"span\",class_=\"u-hide-phablet\"):\n",
    "    team.append(i.text.strip())\n",
    "\n",
    "matches = []\n",
    "for i in soup.find_all(\"td\",class_=\"rankings-block__banner--matches\"):\n",
    "    matches.append(i.text.strip())\n",
    "test = []\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell u-center-text\"):\n",
    "    test.append(i.text.strip())\n",
    "for x,y in enumerate(test):\n",
    "    if x%2==0:\n",
    "        matches.append(y)\n",
    "\n",
    "\n",
    "\n",
    "points = []\n",
    "for i in soup.find_all(\"td\",class_=\"rankings-block__banner--points\"):\n",
    "    points.append(i.text.strip())\n",
    "test = []\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell u-center-text\"):\n",
    "    test.append(i.text.strip())\n",
    "for x,y in enumerate(test):\n",
    "    if x%2!=0:\n",
    "        points.append(y)\n",
    "\n",
    "\n",
    "ratings = []\n",
    "for i in soup.find_all(\"td\",class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "    ratings.append(i.text.strip())\n",
    "\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell u-text-right rating\"):\n",
    "    ratings.append(i.text.strip())\n",
    "        \n",
    "        \n",
    "        \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Team\":team[:10],\"Matches\":matches[:10],\"Points\":points[:10],\"Ratings\":ratings[:10]})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d7061",
   "metadata": {},
   "source": [
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "034f5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "              Team Name Team Ratings\n",
      "0  Natalie Sciver-Brunt  ENG     807\n",
      "1           Beth Mooney  AUS     750\n",
      "2   Chamari Athapaththu   SL     736\n",
      "3       Laura Wolvaardt   SA     727\n",
      "4       Smriti Mandhana  IND     708\n",
      "5          Alyssa Healy  AUS     698\n",
      "6          Ellyse Perry  AUS     697\n",
      "7      Harmanpreet Kaur  IND     694\n",
      "8           Meg Lanning  AUS     662\n",
      "9        Marizanne Kapp   SA     642\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "teamName1 = []\n",
    "for i in soup.find_all(\"div\",class_=\"rankings-block__banner--name-large\"):\n",
    "    teamName1.append(i.text.strip())\n",
    "teamName = teamName1[0]\n",
    "teamName2 = [teamName]\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell rankings-table__name name\"):\n",
    "    teamName2.append(i.text.strip())\n",
    "\n",
    "\n",
    "team1 = []\n",
    "for i in soup.find_all(\"div\",class_=\"rankings-block__banner--nationality\"):\n",
    "    team1.append(i.text.strip())\n",
    "team = team1[0]\n",
    "team2 = [team]\n",
    "for i in soup.find_all(\"span\",class_=\"table-body__logo-text\"):\n",
    "    team2.append(i.text.strip())\n",
    "\n",
    "\n",
    "\n",
    "teamrat1 = []\n",
    "for i in soup.find_all(\"div\",class_=\"rankings-block__banner--rating\"):\n",
    "    teamrat1.append(i.text.strip())\n",
    "teamrat = teamrat1[0]\n",
    "teamrat2 = [teamrat]\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell rating\"):\n",
    "    teamrat2.append(i.text.strip())\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Team Name\":teamName2[:10],\"Team\":team2[:10],\"Ratings\":teamrat2[:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c6f1b",
   "metadata": {},
   "source": [
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6e18abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "              Team Name Team Ratings                 Carrier Ratings\n",
      "0        Marizanne Kapp   SA     385   419 v West Indies, 10/09/2021\n",
      "1      Ashleigh Gardner  AUS     377   391 v West Indies, 08/10/2023\n",
      "2  Natalie Sciver-Brunt  ENG     360     421 v Australia, 18/07/2023\n",
      "3       Hayley Matthews   WI     358       392 v Ireland, 26/06/2023\n",
      "4           Amelia Kerr   NZ     346   356 v West Indies, 25/09/2022\n",
      "5         Deepti Sharma  IND     312  397 v South Africa, 09/10/2019\n",
      "6          Ellyse Perry  AUS     282   548 v West Indies, 11/09/2019\n",
      "7         Jess Jonassen  AUS     227   308 v West Indies, 11/09/2019\n",
      "8         Sophie Devine   NZ     227     305 v Australia, 05/10/2020\n",
      "9              Nida Dar  PAK     224     232 v Australia, 21/01/2023\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "teamName1 = []\n",
    "for i in soup.find_all(\"div\",class_=\"rankings-block__banner--name-large\"):\n",
    "    teamName1.append(i.text.strip())\n",
    "teamName = teamName1[0]\n",
    "teamName2 = [teamName]\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell rankings-table__name name\"):\n",
    "    teamName2.append(i.text.strip())\n",
    "\n",
    "\n",
    "team1 = []\n",
    "for i in soup.find_all(\"div\",class_=\"rankings-block__banner--nationality\"):\n",
    "    team1.append(i.text.strip())\n",
    "team = team1[0]\n",
    "team2 = [team]\n",
    "for i in soup.find_all(\"span\",class_=\"table-body__logo-text\"):\n",
    "    team2.append(i.text.strip())\n",
    "\n",
    "\n",
    "\n",
    "teamrat1 = []\n",
    "for i in soup.find_all(\"div\",class_=\"rankings-block__banner--rating\"):\n",
    "    teamrat1.append(i.text.strip())\n",
    "teamrat = teamrat1[0]\n",
    "teamrat2 = [teamrat]\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell rating\"):\n",
    "    teamrat2.append(i.text.strip())\n",
    "    \n",
    "\n",
    "CarrierRat1 = []\n",
    "for i in soup.find_all(\"span\",class_=\"rankings-block__career-best-text\"):\n",
    "    CarrierRat1.append(i.text.strip())\n",
    "CarrierRat = CarrierRat1[0]\n",
    "CarrierRat2 = [CarrierRat]\n",
    "for i in soup.find_all(\"td\",class_=\"table-body__cell u-text-right u-hide-phablet\"):\n",
    "    CarrierRat2.append(i.text.strip())\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Team Name\":teamName2[:10],\"Team\":team2[:10],\"Ratings\":teamrat2[:10], \"Carrier Ratings\":CarrierRat2[:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf6cdfb",
   "metadata": {},
   "source": [
    "# Question 5 --> Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "98cfbe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "                                             Headline          Time  \\\n",
      "0   Moscow under pressure to protect Jewish commun...    11 Min Ago   \n",
      "1   U.S. is working with Indo-Pacific partners to ...    1 Hour Ago   \n",
      "2   Israeli defense forces expand Gaza ground incu...   2 Hours Ago   \n",
      "3   CNBC Daily Open: Are things about to get more ...   3 Hours Ago   \n",
      "4   European markets open higher despite geopoliti...   3 Hours Ago   \n",
      "5   Is your hotel sustainable? Not if these two th...   3 Hours Ago   \n",
      "6   WTO chief warns Israel-Hamas war could hurt gl...   3 Hours Ago   \n",
      "7   HSBC's after-tax profit surges over 235% year-...   5 Hours Ago   \n",
      "8   Evergrande shares fall 20% to all-time low as ...   5 Hours Ago   \n",
      "9   McDonald's is about to report earnings. Here's...   5 Hours Ago   \n",
      "10  Oil prices slip even as Middle East tensions s...   6 Hours Ago   \n",
      "11  UAW-Ford deal includes $8.1 billion in investm...   8 Hours Ago   \n",
      "12  Standard Chartered-owned crypto firm Zodia lau...   8 Hours Ago   \n",
      "13                 CNBC Daily Open: The perfect storm   9 Hours Ago   \n",
      "14  Asia markets mixed ahead of Japan's BOJ decisi...   9 Hours Ago   \n",
      "15  Amid an AI 'game of thrones,' these China tech...  10 Hours Ago   \n",
      "16  Fund manager says this under-the-radar network...  10 Hours Ago   \n",
      "17  Stock futures rebound slightly with the S&P 50...  10 Hours Ago   \n",
      "18  Chinese consumer stocks that analysts like for...  11 Hours Ago   \n",
      "19  Cramer: This market has put investors in a box...  17 Hours Ago   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/10/29/stock-market-t...  \n",
      "1   https://www.cnbc.com/2023/10/28/analyzing-the-...  \n",
      "2   https://www.cnbc.com/2023/10/27/fed-decision-j...  \n",
      "3   https://www.cnbc.com/2023/10/28/israel-hamas-w...  \n",
      "4   https://www.cnbc.com/2023/10/30/missed-nvidia-...  \n",
      "5   https://www.cnbc.com/2023/10/30/these-chinese-...  \n",
      "6   https://www.cnbc.com/2023/10/29/chinese-consum...  \n",
      "7   https://www.cnbc.com/2023/10/29/earnings-playb...  \n",
      "8   https://www.cnbc.com/2023/10/29/here-are-5-ant...  \n",
      "9   https://www.cnbc.com/2023/10/30/ukraine-war-li...  \n",
      "10  https://www.cnbc.com/2023/10/30/g7-calls-for-i...  \n",
      "11  https://www.cnbc.com/2023/10/28/israel-hamas-w...  \n",
      "12  https://www.cnbc.com/2023/10/27/a-new-hope-for...  \n",
      "13  https://www.cnbc.com/2023/10/26/russias-influe...  \n",
      "14  https://www.cnbc.com/2023/10/27/volvo-cars-ceo...  \n",
      "15  https://www.cnbc.com/2023/10/27/peak-fossil-fu...  \n",
      "16  https://www.cnbc.com/2023/10/24/demand-for-fos...  \n",
      "17  https://www.cnbc.com/2023/10/19/americas-first...  \n",
      "18  https://www.cnbc.com/2023/10/09/the-worlds-lar...  \n",
      "19  https://www.cnbc.com/video/2023/10/30/vietnam-...  \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get('https://www.cnbc.com/world/?region=world')\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "\n",
    "headlines = []\n",
    "\n",
    "for article in soup.find_all('a', class_='LatestNews-headline'):\n",
    "    headline = article.text.strip()\n",
    "    headlines.append(headline)\n",
    "\n",
    "times = []\n",
    "for time in soup.find_all('time', class_='LatestNews-timestamp'):\n",
    "    times.append(time.text.strip())\n",
    "    \n",
    "news_links = []\n",
    "for link in soup.find_all('a', class_='Card-title'):\n",
    "    if link.get('href'):\n",
    "        news_links.append(link.get('href'))\n",
    "\n",
    "df = pd.DataFrame({'Headline': headlines[:20],'Time': times[:20], 'News Link': news_links[:20]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91922f",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "--> Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "15e3591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1   Explanation in artificial intelligence: Insigh...   \n",
      "2              Creativity and artificial intelligence   \n",
      "3   Conflict-based search for optimal multi-agent ...   \n",
      "4   Knowledge graphs as tools for explainable mach...   \n",
      "5   Law and logic: A review from an argumentation ...   \n",
      "6   Between MDPs and semi-MDPs: A framework for te...   \n",
      "7   Explaining individual predictions when feature...   \n",
      "8       Multiple object tracking: A literature review   \n",
      "9   A survey of inverse reinforcement learning: Ch...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11  Explainable AI tools for legal reasoning about...   \n",
      "12            Hard choices in artificial intelligence   \n",
      "13  Assessing the communication gap between AI mod...   \n",
      "14  Explaining black-box classifiers using post-ho...   \n",
      "15  The Hanabi challenge: A new frontier for AI re...   \n",
      "16              Wrappers for feature subset selection   \n",
      "17  Artificial cognition for social human–robot in...   \n",
      "18  A review of possible effects of cognitive bias...   \n",
      "19  The multifaceted impact of Ada Lovelace in the...   \n",
      "20  Robot ethics: Mapping the issues for a mechani...   \n",
      "21          Reward (Mis)design for autonomous driving   \n",
      "22  Planning and acting in partially observable st...   \n",
      "23  What do we want from Explainable Artificial In...   \n",
      "\n",
      "                                               Author            Date  \\\n",
      "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
      "1                                          Tim Miller   February 2019   \n",
      "2                                   Margaret A. Boden     August 1998   \n",
      "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
      "4                      Ilaria Tiddi, Stefan Schlobach    January 2022   \n",
      "5                      Henry Prakken, Giovanni Sartor    October 2015   \n",
      "6     Richard S. Sutton, Doina Precup, Satinder Singh     August 1999   \n",
      "7           Kjersti Aas, Martin Jullum, Anders Løland  September 2021   \n",
      "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
      "9                       Saurabh Arora, Prashant Doshi     August 2021   \n",
      "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
      "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
      "12   Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz   November 2021   \n",
      "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
      "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
      "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
      "16                         Ron Kohavi, George H. John   December 1997   \n",
      "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
      "18    Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz       June 2021   \n",
      "19                             Luigia Carlucci Aiello       June 2016   \n",
      "20             Patrick Lin, Keith Abney, George Bekey      April 2011   \n",
      "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
      "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
      "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
      "\n",
      "                                                 Link  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "papertitle = []\n",
    "for i in soup.find_all(\"a\",class_=\"sc-5smygv-0 fIXTHm\"):\n",
    "    papertitle.append(i.text.strip())\n",
    "\n",
    "author = []\n",
    "for i in soup.find_all(\"span\",class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "    author.append(i.text.strip())\n",
    "\n",
    "date = []\n",
    "for i in soup.find_all(\"span\",class_=\"sc-1thf9ly-2 dvggWt\"):\n",
    "    date.append(i.text.strip())\n",
    "\n",
    "links = []\n",
    "for link in soup.find_all('a', class_='sc-5smygv-0 fIXTHm'):\n",
    "    if link.get('href'):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Paper Title\":papertitle,\"Author\":author,\"Date\":date, \"Link\": news_links})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b1ebf",
   "metadata": {},
   "source": [
    "# Question 7--> Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "de97f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "                                        Name  \\\n",
      "0                              The G.T. Road   \n",
      "1                       Connaught Club House   \n",
      "2                        My Bar Headquarters   \n",
      "3                           Ministry Of Beer   \n",
      "4                               Gin & Juliet   \n",
      "5                                     Sandoz   \n",
      "6                        Unplugged Courtyard   \n",
      "7                                      Local   \n",
      "8                                    Tamasha   \n",
      "9                          The Junkyard Cafe   \n",
      "10                            Openhouse Cafe   \n",
      "11           Ardor 2.1 Restaurant and Lounge   \n",
      "12              Yeti - The Himalayan Kitchen   \n",
      "13                                Farzi Cafe   \n",
      "14                                     Chido   \n",
      "15                               38 Barracks   \n",
      "16                                  Cafe MRP   \n",
      "17  Desi Villagio - Village Theme Restro Bar   \n",
      "18                                Imperfecto   \n",
      "19                               The Embassy   \n",
      "20                                   Berco's   \n",
      "\n",
      "                                        Location  \\\n",
      "0         M-Block,Connaught Place, Central Delhi   \n",
      "1                 Connaught Place, Central Delhi   \n",
      "2                 Connaught Place, Central Delhi   \n",
      "3         M-Block,Connaught Place, Central Delhi   \n",
      "4         F-Block,Connaught Place, Central Delhi   \n",
      "5                 Connaught Place, Central Delhi   \n",
      "6                 Connaught Place, Central Delhi   \n",
      "7   Scindia House,Connaught Place, Central Delhi   \n",
      "8                 Connaught Place, Central Delhi   \n",
      "9                 Connaught Place, Central Delhi   \n",
      "10                Connaught Place, Central Delhi   \n",
      "11                Connaught Place, Central Delhi   \n",
      "12  The Colonnade,Connaught Place, Central Delhi   \n",
      "13                Connaught Place, Central Delhi   \n",
      "14                Connaught Place, Central Delhi   \n",
      "15        M-Block,Connaught Place, Central Delhi   \n",
      "16                Connaught Place, Central Delhi   \n",
      "17                Connaught Place, Central Delhi   \n",
      "18                        Janpath, Central Delhi   \n",
      "19                Connaught Place, Central Delhi   \n",
      "20                Connaught Place, Central Delhi   \n",
      "\n",
      "                                              Cuisine Ratings  \\\n",
      "0                                        North Indian     4.3   \n",
      "1           North Indian, Continental, Asian, Chinese     4.2   \n",
      "2                               North Indian, Chinese       4   \n",
      "3          North Indian, Continental, American, Asian       4   \n",
      "4           Italian, Chinese, North Indian, Fast Food       4   \n",
      "5                           North Indian, Continental       4   \n",
      "6   North Indian, Italian, Chinese, Turkish, Conti...       4   \n",
      "7                    North Indian, Asian, Continental       4   \n",
      "8           Continental, Asian, Italian, North Indian     4.2   \n",
      "9       North Indian, Continental, Chinese, Fast Food     4.1   \n",
      "10                       North Indian, Asian, Italian     4.1   \n",
      "11        North Indian, Chinese, Italian, Continental     4.1   \n",
      "12                                  Tibetan, Nepalese     4.3   \n",
      "13            Modern Indian, Continental, Finger Food     4.1   \n",
      "14  North Indian, Italian, Continental, Asian, Fin...     4.2   \n",
      "15                 North Indian, Chinese, Continental     4.3   \n",
      "16    North Indian, Street Food, Italian, Continental     4.1   \n",
      "17                                       North Indian     4.1   \n",
      "18      Asian, Finger Food, Continental, North Indian     4.1   \n",
      "19  North Indian, European, Fast Food, Italian, Co...     4.1   \n",
      "20                                      Chinese, Thai     4.3   \n",
      "\n",
      "                                             ImageURL  \n",
      "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "13  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "14  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "15  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "16  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "17  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "18  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "19  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "20  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page =requests.get(\"https://www.dineout.co.in/delhi-restaurants/welcome-back\")\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "R_Names = []\n",
    "for i in soup.find_all(\"a\",class_=\"restnt-name ellipsis\"):\n",
    "    R_Names.append(i.text.strip())\n",
    "\n",
    "\n",
    "R_loc = []\n",
    "for i in soup.find_all(\"div\",class_=\"restnt-loc ellipsis\"):\n",
    "    R_loc.append(i.text.strip())\n",
    "\n",
    "cus = []\n",
    "for i in soup.find_all(\"span\",class_=\"double-line-ellipsis\"):\n",
    "    cus.append(i.text.strip())\n",
    "cuis = [entry.split('|')[-1].strip() for entry in cus]\n",
    "    \n",
    "rat = []\n",
    "for i in soup.find_all(\"div\",class_=\"restnt-rating rating-4\"):\n",
    "    rat.append(i.text.strip())\n",
    "    \n",
    "\n",
    "images = []\n",
    "for i in soup.find_all(\"img\",class_=\"no-img\"):\n",
    "    images.append(i[\"data-src\"])\n",
    "\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Name\":R_Names,\"Location\":R_loc, \"Cuisine\":cuis,\"Ratings\":rat,\"ImageURL\": images})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7470807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
